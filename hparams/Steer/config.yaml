# General 
model_name_or_path: ../models/gemma-2-9b
# model_name_or_path: ../hugging_cache/llama-7b
torch_dtype: bfloat16
device: cuda:0
seed: 0
use_chat_template: false
system_prompt: ''  # Adds a system prompt to all method inputs, except for `vector_prompt`, which uses both with and without this prompt to convert it into a vector.

# Generate Vector 
# The `steer_train_hparam_paths` and `steer_vector_output_dirs` are corresponding line by line.
steer_train_hparam_paths:
 - hparams/Steer/caa_hparams/generate_caa.yaml
steer_train_dataset: toxicity
save_vectors: True
steer_vector_output_dirs: 
 - vectors/gemma-2-9b/

# Apply Vector 
# The `apply_steer_hparam_paths` and `steer_vector_load_dir` are corresponding line by line.
apply_steer_hparam_paths:
 - hparams/Steer/caa_hparams/apply_caa.yaml
steer_vector_load_dir: 
 - vectors/gemma-2-9b/toxicity/caa_vector

# Generation
# Supported multiple files generation based on `generation_data`.
# You can choose dataset here, e.g., `- realtoxicityprompts`. 
# Details of the dataset can be found in `hparams/Steer/dataset_format.yaml`.
generation_data: 
 - realtoxicityprompts 
generation_data_size: null  # If set to null, it will use the full dataset.
generation_output_dir: generation/gemma-2-9b/realtoxicityprompts/caa
num_responses: 1
steer_from_end_position: false

generate_orig_output: true

# Model generation parameters - must match Hugging Face parameter names or the vLLM parameter names
# for huggingface generation, see: https://huggingface.co/docs/transformers/main_classes/text_generation
# for vLLM generation, see: https://docs.vllm.com.cn/en/latest/api/inference_params.html
# VLLM generation params example:
# generation_params:
  # max_tokens: 100 #openai-like parameter for vLLMï¼Œif not set, it will be the same as max_new_tokens
  # do_sample: true
  # temperature: 0.9 
  # top_p: 0.9
  
generation_params: 
  max_new_tokens: 100
  do_sample: true
  temperature: 0.9
  top_p: 0.9

# set to true for vLLM generation
vllm_enable: false

